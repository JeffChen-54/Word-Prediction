{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alice', 'adventures', 'in', 'wonderland', 'by', 'lewis', 'carroll', 'the', 'millennium', 'fulcrum', 'edition', '3', 'contents', 'chapter', 'i', 'down', 'the', 'rabbit', 'chapter', 'ii', 'the', 'pool', 'of', 'tears', 'chapter']\n",
      "corpus len:  25320\n"
     ]
    }
   ],
   "source": [
    "corpus = []\n",
    "f = open('alice_in_wonderland.txt','r')\n",
    "while(1):\n",
    "    line =  f.readline()\n",
    "    if len(line) == 0: break\n",
    "    corpus.extend(line.split())\n",
    "        \n",
    "f.close()\n",
    "corpus = ' '.join(corpus)\n",
    "\n",
    "def clean_word(word):\n",
    "    word = word.lower()\n",
    "    for punctuation in ['\"',\"'\",'.',',','-','?','!',';',':','â€”','(',')','[',']']:\n",
    "        word = word.split(punctuation)[0]\n",
    "    return word\n",
    "\n",
    "\n",
    "\n",
    "corpus = [clean_word(word) for word in corpus.split()]\n",
    "corpus = [word for word in corpus if len(word) > 0]\n",
    "print(corpus[:25])\n",
    "D = len(corpus)\n",
    "print('corpus len: ',D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word list size (number of distinct words):  2501\n"
     ]
    }
   ],
   "source": [
    "tokenize = {}\n",
    "wordlist = []\n",
    "token = 0\n",
    "for word in corpus:\n",
    "    if word not in tokenize.keys():\n",
    "        tokenize[word] = token\n",
    "        wordlist.append(word)\n",
    "        token += 1\n",
    "    \n",
    "V = len(wordlist)\n",
    "print('word list size (number of distinct words): ', V)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [9. 1. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# bin how many times a word follows another word\n",
    "counts_2gram = np.zeros((V,V))\n",
    "for i in range(1,len(corpus)):\n",
    "    token_i = tokenize[corpus[i]]\n",
    "    token_im1 = tokenize[corpus[i-1]]\n",
    "    counts_2gram[token_i,token_im1] += 1\n",
    "print(counts_2gram)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('was', 0.0007109004739336493)\n",
      "('queen', 0.0027646129541864135)\n",
      "('cat', 0.00019747235387045816)\n",
      "('turtle', 0.0022511848341232226)\n",
      "('and', 0.00015797788309636652)\n",
      "('said', 0.00015797788309636652)\n",
      "The accuracy for n=1\n",
      "0.2453493423910897\n"
     ]
    }
   ],
   "source": [
    "#past word as feature\n",
    "\n",
    "posterior_1word = np.zeros((V, V))\n",
    "prior = np.zeros(V)\n",
    "\n",
    "def get_likelihood_2gram(word):\n",
    "    i = tokenize[word]\n",
    "    posterior_1word = counts_2gram.copy()\n",
    "    posterior_1word = posterior_1word.T\n",
    "    likelihood = posterior_1word[i]\n",
    "    \n",
    "    count = 0\n",
    "    \n",
    "    for word in corpus:\n",
    "        if i == tokenize[word]:\n",
    "            count = count+1\n",
    "    \n",
    "    count = count/D\n",
    "    \n",
    "    sum = 0\n",
    "    for i in range(V):\n",
    "        sum += likelihood[i]\n",
    "        \n",
    "    for i in range(V):\n",
    "        likelihood[i] = likelihood[i]/sum * count\n",
    "    return(likelihood)\n",
    "def pred_2gram(word):\n",
    "    likelihood = get_likelihood_2gram(word)\n",
    "    i = np.argmax(likelihood)\n",
    "    return(wordlist[i], likelihood[i])\n",
    "\n",
    "def classification_accuracy():\n",
    "    count = 0\n",
    "    next_word = ''\n",
    "    for word in corpus:\n",
    "        if next_word == word:\n",
    "            count = count + 1\n",
    "        likelihood = get_likelihood_2gram(word)\n",
    "        i = np.argmax(likelihood)\n",
    "        next_word = wordlist[i]\n",
    "    return(count/(len(corpus)-1))\n",
    "        \n",
    "        \n",
    "print(pred_2gram('alice'))\n",
    "print(pred_2gram('the'))\n",
    "print(pred_2gram('cheshire'))\n",
    "print(pred_2gram('mock'))\n",
    "print(pred_2gram('cat'))\n",
    "print(pred_2gram('turtle'))\n",
    "print(\"The accuracy for n=1\")\n",
    "print(classification_accuracy())\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('miles', 1.3164823591363875e-05)\n",
      "('girl', 1.8513033175355453e-06)\n",
      "('well', 4.7027976363754134e-11)\n",
      "Answer for 4c\n",
      "with\n",
      "this\n",
      "as\n",
      "she\n",
      "could\n",
      "guess\n",
      "she\n",
      "was\n",
      "now\n",
      "about\n",
      "two\n",
      "feet\n",
      "high\n",
      "even\n",
      "then\n",
      "they\n",
      "walked\n",
      "off\n",
      "together\n",
      "alice\n",
      "heard\n",
      "a\n",
      "little\n",
      "pattering\n",
      "of\n",
      "Answer for 4d\n",
      "with\n",
      "this\n",
      "said\n",
      "the\n",
      "king\n",
      "saves\n",
      "a\n",
      "world\n",
      "of\n",
      "trouble\n",
      "you\n",
      "know\n",
      "you\n",
      "know\n",
      "why\n",
      "do\n",
      "you\n",
      "call\n",
      "it\n",
      "sad\n",
      "and\n",
      "she\n",
      "kept\n",
      "on\n",
      "puzzling\n",
      "The accuracy for n=3\n",
      "0.7499703756369238\n",
      "The accuracy for n=5\n",
      "0.9401935611297649\n",
      "The accuracy for n=10\n",
      "0.9960489924930858\n"
     ]
    }
   ],
   "source": [
    "#past 2 words as features\n",
    "\n",
    "posterior_2words = np.zeros((V, V))\n",
    "\n",
    "pre2_gram = np.zeros((V,V))\n",
    "for i in range(2,len(corpus)):\n",
    "    token_i = tokenize[corpus[i]]\n",
    "    token_im1 = tokenize[corpus[i-2]]\n",
    "    pre2_gram[token_i,token_im1] += 1\n",
    "\n",
    "pre3_gram = np.zeros((V,V))\n",
    "for i in range(3,len(corpus)):\n",
    "    token_i = tokenize[corpus[i]]\n",
    "    token_im1 = tokenize[corpus[i-3]]\n",
    "    pre3_gram[token_i,token_im1] += 1\n",
    "    \n",
    "pre4_gram = np.zeros((V,V))\n",
    "for i in range(4,len(corpus)):\n",
    "    token_i = tokenize[corpus[i]]\n",
    "    token_im1 = tokenize[corpus[i-4]]\n",
    "    pre4_gram[token_i,token_im1] += 1\n",
    "    \n",
    "pre5_gram = np.zeros((V,V))\n",
    "for i in range(5,len(corpus)):\n",
    "    token_i = tokenize[corpus[i]]\n",
    "    token_im1 = tokenize[corpus[i-5]]\n",
    "    pre5_gram[token_i,token_im1] += 1\n",
    "    \n",
    "pre6_gram = np.zeros((V,V))\n",
    "for i in range(6,len(corpus)):\n",
    "    token_i = tokenize[corpus[i]]\n",
    "    token_im1 = tokenize[corpus[i-6]]\n",
    "    pre6_gram[token_i,token_im1] += 1\n",
    "    \n",
    "pre7_gram = np.zeros((V,V))\n",
    "for i in range(7,len(corpus)):\n",
    "    token_i = tokenize[corpus[i]]\n",
    "    token_im1 = tokenize[corpus[i-7]]\n",
    "    pre7_gram[token_i,token_im1] += 1\n",
    "    \n",
    "pre8_gram = np.zeros((V,V))\n",
    "for i in range(8,len(corpus)):\n",
    "    token_i = tokenize[corpus[i]]\n",
    "    token_im1 = tokenize[corpus[i-8]]\n",
    "    pre8_gram[token_i,token_im1] += 1\n",
    "    \n",
    "pre9_gram = np.zeros((V,V))\n",
    "for i in range(9,len(corpus)):\n",
    "    token_i = tokenize[corpus[i]]\n",
    "    token_im1 = tokenize[corpus[i-9]]\n",
    "    pre9_gram[token_i,token_im1] += 1\n",
    "    \n",
    "pre10_gram = np.zeros((V,V))\n",
    "for i in range(10,len(corpus)):\n",
    "    token_i = tokenize[corpus[i]]\n",
    "    token_im1 = tokenize[corpus[i-10]]\n",
    "    pre10_gram[token_i,token_im1] += 1\n",
    "    \n",
    "def get_likelihood_3gram(word2ago,word1ago):\n",
    "    i1 = tokenize[word1ago]\n",
    "    i2 = tokenize[word2ago]\n",
    "    posterior_1word = counts_2gram.copy()\n",
    "    posterior_2word = pre2_gram.copy()\n",
    "    \n",
    "    p1 = posterior_1word[i1].copy()\n",
    "    p2 = posterior_2word[i2].copy()\n",
    "    \n",
    "    c = np.zeros(V)\n",
    "    \n",
    "    posterior_1word = posterior_1word.T\n",
    "    posterior_2word = posterior_2word.T\n",
    "    \n",
    "    likelihood1 = posterior_1word[i1]\n",
    "    likelihood2 = posterior_2word[i2]\n",
    "    likelihood = np.zeros(V)\n",
    "    \n",
    "    count1 = 0\n",
    "    count2 = 0\n",
    "    \n",
    "    for word in corpus:\n",
    "        if i1 == tokenize[word]:\n",
    "            count1 = count1+1\n",
    "    \n",
    "    for word in corpus:\n",
    "        if i2 == tokenize[word]:\n",
    "            count2 = count2+1\n",
    "        c[tokenize[word]] = c[tokenize[word]] + 1\n",
    "        \n",
    "    \n",
    "    count1 = count1/D\n",
    "    count2 = count2/D\n",
    "    \n",
    "    sum1 = 0\n",
    "    sum2 = 0\n",
    "    sc = 0\n",
    "    s1 = 0\n",
    "    s2 = 0\n",
    "    \n",
    "    for i in range(V):\n",
    "        sum1 += likelihood1[i]\n",
    "        sum2 += likelihood2[i]\n",
    "        s1 += p1[i]\n",
    "        s2 += p2[i]\n",
    "        sc += c[i]\n",
    "        \n",
    "    for i in range(V):\n",
    "        likelihood1[i] = likelihood1[i]/sum1 * count1 \n",
    "        likelihood2[i] = likelihood2[i]/sum2 * count2/(c[i]/sc)\n",
    "        likelihood[i] = likelihood1[i] * likelihood2[i]\n",
    "    \n",
    "    return likelihood\n",
    "def pred_3gram(word2ago,word1ago):\n",
    "    likelihood = get_likelihood_3gram(word2ago,word1ago)\n",
    "    i = np.argmax(likelihood)\n",
    "    return wordlist[i], likelihood[i]\n",
    "#print(pred_3gram('pack','of'))\n",
    "#print(pred_3gram('the','mad'))\n",
    "#print(pred_3gram('she','jumped'))\n",
    "#print(pred_3gram('four','thousand'))\n",
    "\n",
    "\n",
    "def get_likelihood_4gram(word4ago,word3ago,word2ago,word1ago):\n",
    "    i1 = tokenize[word1ago]\n",
    "    i2 = tokenize[word2ago]\n",
    "    i3 = tokenize[word3ago]\n",
    "    i4 = tokenize[word4ago]\n",
    "    posterior_1word = counts_2gram.copy()\n",
    "    posterior_2word = pre2_gram.copy()\n",
    "    posterior_3word = pre3_gram.copy()\n",
    "    posterior_4word = pre4_gram.copy()\n",
    "    \n",
    "    posterior_1word = posterior_1word.T\n",
    "    posterior_2word = posterior_2word.T\n",
    "    posterior_3word = posterior_3word.T\n",
    "    posterior_4word = posterior_4word.T\n",
    "    \n",
    "    c = np.zeros(V)\n",
    "\n",
    "    \n",
    "    likelihood1 = posterior_1word[i1]\n",
    "    likelihood2 = posterior_2word[i2]\n",
    "    likelihood3 = posterior_3word[i3]\n",
    "    likelihood4 = posterior_4word[i4]\n",
    "\n",
    "    likelihood = np.zeros(V)\n",
    "    \n",
    "    count1 = 0\n",
    "    count2 = 0\n",
    "    count3 = 0\n",
    "    count4 = 0\n",
    "\n",
    "    \n",
    "    for word in corpus:\n",
    "        c[tokenize[word]] = c[tokenize[word]] + 1\n",
    "        if i1 == tokenize[word]:\n",
    "            count1 = count1+1\n",
    "    \n",
    "    for word in corpus:\n",
    "        if i2 == tokenize[word]:\n",
    "            count2 = count2+1\n",
    "\n",
    "    for word in corpus:\n",
    "        if i3 == tokenize[word]:\n",
    "            count3 = count3+1\n",
    "            \n",
    "    for word in corpus:\n",
    "        if i4 == tokenize[word]:\n",
    "            count4 = count4+1\n",
    "    \n",
    "    count1 = count1/D\n",
    "    count2 = count2/D\n",
    "    count3 = count3/D\n",
    "    count4 = count4/D\n",
    "\n",
    "    \n",
    "    sum1 = 0\n",
    "    sum2 = 0\n",
    "    sum3 = 0\n",
    "    sum4 = 0\n",
    "    sc = 0\n",
    "    \n",
    "    for i in range(V):\n",
    "        sum1 += likelihood1[i]\n",
    "        sum2 += likelihood2[i]\n",
    "        sum3 += likelihood3[i]\n",
    "        sum4 += likelihood4[i]\n",
    "        sc += c[i]\n",
    "        \n",
    "    for i in range(V):\n",
    "        likelihood1[i] = likelihood1[i]/sum1 * count1\n",
    "        likelihood2[i] = likelihood2[i]/sum2 * count2\n",
    "        likelihood3[i] = likelihood3[i]/sum3 * count3\n",
    "        likelihood4[i] = likelihood4[i]/sum4 * count4\n",
    "        likelihood[i] = likelihood1[i] * likelihood2[i] * likelihood3[i] * likelihood4[i] /(c[i]/sc)/(c[i]/sc)/(c[i]/sc)\n",
    "    \n",
    "    return likelihood\n",
    "def pred_4gram(word4ago,word3ago,word2ago,word1ago):\n",
    "    likelihood = get_likelihood_4gram(word4ago,word3ago,word2ago,word1ago)\n",
    "    i = np.argmax(likelihood)\n",
    "    return wordlist[i], likelihood[i]\n",
    "\n",
    "#print(pred_4gram ('what', 'an', 'ignorant', 'little'))\n",
    "    \n",
    "def get_likelihood_5gram(word5ago,word4ago,word3ago,word2ago,word1ago):\n",
    "    i1 = tokenize[word1ago]\n",
    "    i2 = tokenize[word2ago]\n",
    "    i3 = tokenize[word3ago]\n",
    "    i4 = tokenize[word4ago]\n",
    "    i5 = tokenize[word5ago]\n",
    "    posterior_1word = counts_2gram.copy()\n",
    "    posterior_2word = pre2_gram.copy()\n",
    "    posterior_3word = pre3_gram.copy()\n",
    "    posterior_4word = pre4_gram.copy()\n",
    "    posterior_5word = pre5_gram.copy()\n",
    "    \n",
    "    posterior_1word = posterior_1word.T\n",
    "    posterior_2word = posterior_2word.T\n",
    "    posterior_3word = posterior_3word.T\n",
    "    posterior_4word = posterior_4word.T\n",
    "    posterior_5word = posterior_5word.T\n",
    "    \n",
    "    c = np.zeros(V)\n",
    "\n",
    "    \n",
    "    likelihood1 = posterior_1word[i1]\n",
    "    likelihood2 = posterior_2word[i2]\n",
    "    likelihood3 = posterior_3word[i3]\n",
    "    likelihood4 = posterior_4word[i4]\n",
    "    likelihood5 = posterior_5word[i5]\n",
    "\n",
    "    likelihood = np.zeros(V)\n",
    "    \n",
    "    count1 = 0\n",
    "    count2 = 0\n",
    "    count3 = 0\n",
    "    count4 = 0\n",
    "    count5 = 0\n",
    "\n",
    "    \n",
    "    for word in corpus:\n",
    "        c[tokenize[word]] = c[tokenize[word]] + 1\n",
    "        if i1 == tokenize[word]:\n",
    "            count1 = count1+1\n",
    "        if i2 == tokenize[word]:\n",
    "            count2 = count2+1\n",
    "        if i3 == tokenize[word]:\n",
    "            count3 = count3+1\n",
    "        if i4 == tokenize[word]:\n",
    "            count4 = count4+1\n",
    "        if i5 == tokenize[word]:\n",
    "            count5 = count5+1\n",
    "    \n",
    "    \n",
    "    count1 = count1/D\n",
    "    count2 = count2/D\n",
    "    count3 = count3/D\n",
    "    count4 = count4/D\n",
    "    count5 = count5/D\n",
    "\n",
    "    \n",
    "    sum1 = 0\n",
    "    sum2 = 0\n",
    "    sum3 = 0\n",
    "    sum4 = 0\n",
    "    sum5 = 0\n",
    "    sc = 0\n",
    "    \n",
    "    for i in range(V):\n",
    "        sum1 += likelihood1[i]\n",
    "        sum2 += likelihood2[i]\n",
    "        sum3 += likelihood3[i]\n",
    "        sum4 += likelihood4[i]\n",
    "        sum5 += likelihood5[i]\n",
    "        sc += c[i]\n",
    "        \n",
    "    \n",
    "        \n",
    "    for i in range(V):\n",
    "        likelihood1[i] = likelihood1[i]/sum1 * count1\n",
    "        likelihood2[i] = likelihood2[i]/sum2 * count2\n",
    "        likelihood3[i] = likelihood3[i]/sum3 * count3\n",
    "        likelihood4[i] = likelihood4[i]/sum4 * count4\n",
    "        likelihood5[i] = likelihood5[i]/sum5 * count5\n",
    "        likelihood[i] = likelihood1[i] * likelihood2[i] * likelihood3[i] * likelihood4[i] * likelihood5[i] /(c[i]/sc)/(c[i]/sc)/(c[i]/sc)/(c[i]/sc)\n",
    "    \n",
    "    return likelihood\n",
    "def pred_5gram(word5ago,word4ago,word3ago,word2ago,word1ago):\n",
    "    likelihood = get_likelihood_5gram(word5ago,word4ago,word3ago,word2ago,word1ago)\n",
    "    i = np.argmax(likelihood)\n",
    "    return wordlist[i], likelihood[i]\n",
    "\n",
    "def get_likelihood_t3gram(word3ago,word2ago,word1ago):\n",
    "    i1 = tokenize[word1ago]\n",
    "    i2 = tokenize[word2ago]\n",
    "    i3 = tokenize[word3ago]\n",
    "\n",
    "    posterior_1word = counts_2gram.copy()\n",
    "    posterior_2word = pre2_gram.copy()\n",
    "    posterior_3word = pre3_gram.copy()\n",
    "\n",
    "    \n",
    "    posterior_1word = posterior_1word.T\n",
    "    posterior_2word = posterior_2word.T\n",
    "    posterior_3word = posterior_3word.T\n",
    "\n",
    "    \n",
    "    c = np.zeros(V)\n",
    "\n",
    "    \n",
    "    likelihood1 = posterior_1word[i1]\n",
    "    likelihood2 = posterior_2word[i2]\n",
    "    likelihood3 = posterior_3word[i3]\n",
    "\n",
    "    likelihood = np.zeros(V)\n",
    "    \n",
    "    count1 = 0\n",
    "    count2 = 0\n",
    "    count3 = 0\n",
    "\n",
    "    \n",
    "    for word in corpus:\n",
    "        c[tokenize[word]] = c[tokenize[word]] + 1\n",
    "        if i1 == tokenize[word]:\n",
    "            count1 = count1+1\n",
    "        if i2 == tokenize[word]:\n",
    "            count2 = count2+1\n",
    "        if i3 == tokenize[word]:\n",
    "            count3 = count3+1\n",
    "    \n",
    "    \n",
    "    count1 = count1/D\n",
    "    count2 = count2/D\n",
    "    count3 = count3/D\n",
    "\n",
    "    \n",
    "    sum1 = 0\n",
    "    sum2 = 0\n",
    "    sum3 = 0\n",
    "    sc = 0\n",
    "    \n",
    "    for i in range(V):\n",
    "        sum1 += likelihood1[i]\n",
    "        sum2 += likelihood2[i]\n",
    "        sum3 += likelihood3[i]\n",
    "        sc += c[i]\n",
    "        \n",
    "    for i in range(V):\n",
    "        likelihood1[i] = likelihood1[i]/sum1 * count1\n",
    "        likelihood2[i] = likelihood2[i]/sum2 * count2\n",
    "        likelihood3[i] = likelihood3[i]/sum3 * count3\n",
    "        likelihood[i] = likelihood1[i] * likelihood2[i] * likelihood3[i] /(c[i]/sc)/(c[i]/sc)\n",
    "    \n",
    "    return likelihood\n",
    "def pred_t3gram(word3ago,word2ago,word1ago):\n",
    "    likelihood = get_likelihood_t3gram(word3ago,word2ago,word1ago)\n",
    "    i = np.argmax(likelihood)\n",
    "    return wordlist[i], likelihood[i]\n",
    "\n",
    "#print(pred_t3gram('the', 'mad', 'hatter'))\n",
    "def text_gene(word3ago,word2ago,word1ago):\n",
    "    likelihood = get_likelihood_t3gram(word3ago,word2ago,word1ago)\n",
    "    a = word3ago\n",
    "    b = word2ago\n",
    "    c = word1ago\n",
    "    for i in range(25):\n",
    "        i = np.argmax(likelihood)\n",
    "        print(wordlist[i])\n",
    "        a = b\n",
    "        b = c\n",
    "        c = wordlist[i]\n",
    "        likelihood = get_likelihood_t3gram(a,b,c)\n",
    "\n",
    "def text_gene1(word3ago,word2ago,word1ago):\n",
    "    likelihood = get_likelihood_t3gram(word3ago,word2ago,word1ago)\n",
    "    a = word3ago\n",
    "    b = word2ago\n",
    "    c = word1ago\n",
    "    for i in range(25):\n",
    "        a = b\n",
    "        b = c\n",
    "        c = random.choices(wordlist, weights = likelihood, k = 1)\n",
    "        c = ' '.join(c)\n",
    "        print(c)\n",
    "        likelihood = get_likelihood_t3gram(a,b,c)\n",
    "    \n",
    "#text_gene('the', 'mad', 'hatter')\n",
    "\n",
    "#print(pred_5gram('falling', 'down', 'a', 'very', 'deep'))\n",
    "#text_gene1('the', 'mad', 'hatter')\n",
    "\n",
    "def get_likelihood_10gram(word10ago,word9ago,word8ago,word7ago,word6ago,word5ago,word4ago,word3ago,word2ago,word1ago):\n",
    "    i1 = tokenize[word1ago]\n",
    "    i2 = tokenize[word2ago]\n",
    "    i3 = tokenize[word3ago]\n",
    "    i4 = tokenize[word4ago]\n",
    "    i5 = tokenize[word5ago]\n",
    "    i6 = tokenize[word6ago]\n",
    "    i7 = tokenize[word7ago]\n",
    "    i8 = tokenize[word8ago]\n",
    "    i9 = tokenize[word9ago]\n",
    "    i10 = tokenize[word10ago]\n",
    "    \n",
    "    posterior_1word = counts_2gram.copy()\n",
    "    posterior_2word = pre2_gram.copy()\n",
    "    posterior_3word = pre3_gram.copy()\n",
    "    posterior_4word = pre4_gram.copy()\n",
    "    posterior_5word = pre5_gram.copy()\n",
    "    posterior_6word = pre6_gram.copy()\n",
    "    posterior_7word = pre7_gram.copy()\n",
    "    posterior_8word = pre8_gram.copy()\n",
    "    posterior_9word = pre9_gram.copy()\n",
    "    posterior_10word = pre10_gram.copy()\n",
    "    \n",
    "    posterior_1word = posterior_1word.T\n",
    "    posterior_2word = posterior_2word.T\n",
    "    posterior_3word = posterior_3word.T\n",
    "    posterior_4word = posterior_4word.T\n",
    "    posterior_5word = posterior_5word.T\n",
    "    posterior_6word = posterior_6word.T\n",
    "    posterior_7word = posterior_7word.T\n",
    "    posterior_8word = posterior_8word.T\n",
    "    posterior_9word = posterior_9word.T\n",
    "    posterior_10word = posterior_10word.T\n",
    "    \n",
    "    c = np.zeros(V)\n",
    "\n",
    "    \n",
    "    likelihood1 = posterior_1word[i1]\n",
    "    likelihood2 = posterior_2word[i2]\n",
    "    likelihood3 = posterior_3word[i3]\n",
    "    likelihood4 = posterior_4word[i4]\n",
    "    likelihood5 = posterior_5word[i5]\n",
    "    likelihood6 = posterior_6word[i6]\n",
    "    likelihood7 = posterior_7word[i7]\n",
    "    likelihood8 = posterior_8word[i8]\n",
    "    likelihood9 = posterior_9word[i9]\n",
    "    likelihood10 = posterior_10word[i10]\n",
    "\n",
    "    likelihood = np.zeros(V)\n",
    "    \n",
    "    count1 = 0\n",
    "    count2 = 0\n",
    "    count3 = 0\n",
    "    count4 = 0\n",
    "    count5 = 0\n",
    "    count6 = 0\n",
    "    count7 = 0\n",
    "    count8 = 0\n",
    "    count9 = 0\n",
    "    count10 = 0\n",
    "\n",
    "    \n",
    "    for word in corpus:\n",
    "        c[tokenize[word]] = c[tokenize[word]] + 1\n",
    "        if i1 == tokenize[word]:\n",
    "            count1 = count1+1\n",
    "        if i2 == tokenize[word]:\n",
    "            count2 = count2+1\n",
    "        if i3 == tokenize[word]:\n",
    "            count3 = count3+1\n",
    "        if i4 == tokenize[word]:\n",
    "            count4 = count4+1\n",
    "        if i5 == tokenize[word]:\n",
    "            count5 = count5+1\n",
    "        if i6 == tokenize[word]:\n",
    "            count6 = count6+1\n",
    "        if i7 == tokenize[word]:\n",
    "            count7 = count7+1\n",
    "        if i8 == tokenize[word]:\n",
    "            count8 = count8+1\n",
    "        if i9 == tokenize[word]:\n",
    "            count9 = count9+1\n",
    "        if i10 == tokenize[word]:\n",
    "            count10 = count10+1\n",
    "    \n",
    "    \n",
    "    count1 = count1/D\n",
    "    count2 = count2/D\n",
    "    count3 = count3/D\n",
    "    count4 = count4/D\n",
    "    count5 = count5/D\n",
    "    count6 = count6/D\n",
    "    count7 = count7/D\n",
    "    count8 = count8/D\n",
    "    count9 = count9/D\n",
    "    count10 = count10/D\n",
    "\n",
    "    \n",
    "    sum1 = 0\n",
    "    sum2 = 0\n",
    "    sum3 = 0\n",
    "    sum4 = 0\n",
    "    sum5 = 0\n",
    "    sum6 = 0\n",
    "    sum7 = 0\n",
    "    sum8 = 0\n",
    "    sum9 = 0\n",
    "    sum10 = 0\n",
    "    \n",
    "    sc = 0\n",
    "    \n",
    "    for i in range(V):\n",
    "        sum1 += likelihood1[i]\n",
    "        sum2 += likelihood2[i]\n",
    "        sum3 += likelihood3[i]\n",
    "        sum4 += likelihood4[i]\n",
    "        sum5 += likelihood5[i]\n",
    "        sum6 += likelihood6[i]\n",
    "        sum7 += likelihood7[i]\n",
    "        sum8 += likelihood8[i]\n",
    "        sum9 += likelihood9[i]\n",
    "        sum10 += likelihood10[i]\n",
    "        \n",
    "        sc += c[i]\n",
    "        \n",
    "    \n",
    "        \n",
    "    for i in range(V):\n",
    "        likelihood1[i] = likelihood1[i]/sum1 * count1\n",
    "        likelihood2[i] = likelihood2[i]/sum2 * count2\n",
    "        likelihood3[i] = likelihood3[i]/sum3 * count3\n",
    "        likelihood4[i] = likelihood4[i]/sum4 * count4\n",
    "        likelihood5[i] = likelihood5[i]/sum5 * count5\n",
    "        likelihood6[i] = likelihood6[i]/sum6 * count6\n",
    "        likelihood7[i] = likelihood7[i]/sum7 * count7\n",
    "        likelihood8[i] = likelihood8[i]/sum8 * count8\n",
    "        likelihood9[i] = likelihood9[i]/sum9 * count9\n",
    "        likelihood10[i] = likelihood10[i]/sum10 * count10\n",
    "        \n",
    "        likelihood[i] = likelihood1[i] * likelihood2[i] * likelihood3[i] * likelihood4[i] * likelihood5[i] * likelihood6[i]* likelihood7[i]* likelihood8[i]* likelihood9[i]* likelihood10[i]/(c[i]/sc)/(c[i]/sc)/(c[i]/sc)/(c[i]/sc)/(c[i]/sc)/(c[i]/sc)/(c[i]/sc)/(c[i]/sc)/(c[i]/sc)\n",
    "    \n",
    "    return likelihood\n",
    "\n",
    "def classification_n3accuracy():\n",
    "    count = 0\n",
    "    next_word = ''\n",
    "    a = corpus[0]\n",
    "    b = corpus[1]\n",
    "    c = corpus[2]\n",
    "    d = 0\n",
    "    \n",
    "    for word in corpus:\n",
    "        if d < 3:\n",
    "            d +=1\n",
    "            continue\n",
    "\n",
    "        likelihood = get_likelihood_t3gram(a,b,c)\n",
    "        i = np.argmax(likelihood)\n",
    "        a = b\n",
    "        b = c\n",
    "        c = wordlist[i]\n",
    "        if c == word:\n",
    "            count = count + 1\n",
    "        c = word\n",
    "    return(count/(len(corpus)-3))\n",
    "    \n",
    "\n",
    "\n",
    "def classification_n5accuracy():\n",
    "    count = 0\n",
    "    next_word = ''\n",
    "    a = corpus[0]\n",
    "    b = corpus[1]\n",
    "    c = corpus[2]\n",
    "    d = corpus[3]\n",
    "    e = corpus[4]\n",
    "    f = 0\n",
    "    \n",
    "    for word in corpus:\n",
    "        if f < 5:\n",
    "            f +=1\n",
    "            continue\n",
    "        likelihood = get_likelihood_5gram(a,b,c,d,e)\n",
    "        i = np.argmax(likelihood)\n",
    "        a = b\n",
    "        b = c\n",
    "        c = d\n",
    "        d = e\n",
    "        e = wordlist[i]\n",
    "        if e == word:\n",
    "            count = count + 1\n",
    "        e = word\n",
    "    return(count/(len(corpus)-5))\n",
    "    \n",
    "\n",
    "\n",
    "def classification_n10accuracy():\n",
    "    count = 0\n",
    "    next_word = ''\n",
    "    a = corpus[0]\n",
    "    b = corpus[1]\n",
    "    c = corpus[2]\n",
    "    d = corpus[3]\n",
    "    e = corpus[4]\n",
    "    f = corpus[5]\n",
    "    g = corpus[6]\n",
    "    h = corpus[7]\n",
    "    i = corpus[8]\n",
    "    j = corpus[9]\n",
    "    k = 0\n",
    "    \n",
    "    for word in corpus:\n",
    "        if k < 10:\n",
    "            k +=1\n",
    "            continue\n",
    "        likelihood = get_likelihood_10gram(a,b,c,d,e,f,g,h,i,j)\n",
    "        ii = np.argmax(likelihood)\n",
    "        a = b\n",
    "        b = c\n",
    "        c = d\n",
    "        d = e\n",
    "        e = f\n",
    "        f = g\n",
    "        g = h\n",
    "        h = i\n",
    "        i = j\n",
    "        j = wordlist[ii]\n",
    "        if j == word:\n",
    "            count = count + 1\n",
    "        j = word\n",
    "    return(count/(len(corpus)-10))\n",
    "    \n",
    "#print(pred_3gram('pack','of'))\n",
    "#print(pred_3gram('the','mad'))\n",
    "#print(pred_3gram('she','jumped'))\n",
    "print(pred_3gram('four','thousand'))\n",
    "#print(pred_t3gram('the', 'mad', 'hatter'))\n",
    "print(pred_4gram ('what', 'an', 'ignorant', 'little'))\n",
    "print(pred_5gram('falling', 'down', 'a', 'very', 'deep'))\n",
    "print(\"Answer for 4c\")\n",
    "text_gene('the', 'mad', 'hatter')\n",
    "print(\"Answer for 4d\")\n",
    "text_gene1('the', 'mad', 'hatter')\n",
    "print(\"The accuracy for n=3\")\n",
    "print(classification_n3accuracy())\n",
    "print(\"The accuracy for n=5\")\n",
    "print(classification_n5accuracy())\n",
    "print(\"The accuracy for n=10\")\n",
    "print(classification_n10accuracy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "with\n",
      "this\n",
      "as\n",
      "she\n",
      "was\n",
      "shrinking\n",
      "rapidly\n",
      "so\n",
      "she\n",
      "set\n",
      "to\n",
      "work\n",
      "throwing\n",
      "everything\n",
      "within\n",
      "her\n",
      "reach\n",
      "at\n",
      "the\n",
      "duchess\n",
      "and\n",
      "seemed\n",
      "ready\n",
      "to\n",
      "agree\n"
     ]
    }
   ],
   "source": [
    "def text_gene1(word3ago,word2ago,word1ago):\n",
    "    likelihood = get_likelihood_t3gram(word3ago,word2ago,word1ago)\n",
    "    a = word3ago\n",
    "    b = word2ago\n",
    "    c = word1ago\n",
    "    for i in range(25):\n",
    "        a = b\n",
    "        b = c\n",
    "        c = random.choices(wordlist, weights = likelihood, k = 1)\n",
    "        c = ' '.join(c)\n",
    "        print(c)\n",
    "        likelihood = get_likelihood_t3gram(a,b,c)\n",
    "        \n",
    "text_gene1('the', 'mad', 'hatter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
